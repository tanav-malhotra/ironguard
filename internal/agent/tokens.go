package agent

import (
	"sync"
)

// TokenUsage tracks token usage for the session.
type TokenUsage struct {
	mu sync.RWMutex
	
	// Current context
	InputTokens  int // Tokens in current context (messages)
	OutputTokens int // Tokens generated by AI in current turn
	
	// Session totals
	TotalInputTokens  int // Total input tokens used this session
	TotalOutputTokens int // Total output tokens used this session
	
	// Summarization stats
	TokensSavedBySummary int // Tokens saved by summarization
	SummaryCount         int // Number of times context was summarized
	
	// Model context limits (approximate)
	ContextLimit int // Max context window for current model
}

// NewTokenUsage creates a new token usage tracker.
func NewTokenUsage() *TokenUsage {
	return &TokenUsage{
		ContextLimit: 200000, // Default to 200K (Claude's limit)
	}
}

// SetContextLimit sets the context limit for the current model.
func (t *TokenUsage) SetContextLimit(limit int) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.ContextLimit = limit
}

// AddInput records input tokens used.
func (t *TokenUsage) AddInput(tokens int) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.InputTokens = tokens
	t.TotalInputTokens += tokens
}

// AddOutput records output tokens generated.
func (t *TokenUsage) AddOutput(tokens int) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.OutputTokens += tokens
	t.TotalOutputTokens += tokens
}

// RecordSummary records that a summarization occurred.
func (t *TokenUsage) RecordSummary(tokensSaved int) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.TokensSavedBySummary += tokensSaved
	t.SummaryCount++
}

// GetCurrentContext returns current context token count.
func (t *TokenUsage) GetCurrentContext() int {
	t.mu.RLock()
	defer t.mu.RUnlock()
	return t.InputTokens
}

// GetSessionTotal returns total tokens used this session.
func (t *TokenUsage) GetSessionTotal() int {
	t.mu.RLock()
	defer t.mu.RUnlock()
	return t.TotalInputTokens + t.TotalOutputTokens
}

// GetStats returns all token statistics.
func (t *TokenUsage) GetStats() TokenStats {
	t.mu.RLock()
	defer t.mu.RUnlock()
	return TokenStats{
		CurrentContext:       t.InputTokens,
		ContextLimit:         t.ContextLimit,
		ContextPercentage:    float64(t.InputTokens) / float64(t.ContextLimit) * 100,
		TotalInputTokens:     t.TotalInputTokens,
		TotalOutputTokens:    t.TotalOutputTokens,
		TotalTokens:          t.TotalInputTokens + t.TotalOutputTokens,
		TokensSavedBySummary: t.TokensSavedBySummary,
		SummaryCount:         t.SummaryCount,
	}
}

// Reset resets the session token counts.
func (t *TokenUsage) Reset() {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.InputTokens = 0
	t.OutputTokens = 0
	t.TotalInputTokens = 0
	t.TotalOutputTokens = 0
	t.TokensSavedBySummary = 0
	t.SummaryCount = 0
}

// TokenStats contains token usage statistics.
type TokenStats struct {
	CurrentContext       int     // Current context size in tokens
	ContextLimit         int     // Maximum context window
	ContextPercentage    float64 // Percentage of context used
	TotalInputTokens     int     // Total input tokens this session
	TotalOutputTokens    int     // Total output tokens this session
	TotalTokens          int     // Total tokens (input + output)
	TokensSavedBySummary int     // Tokens saved by summarization
	SummaryCount         int     // Number of summarizations
}

// EstimateTokens estimates token count from text (rough: ~4 chars per token).
func EstimateTokens(text string) int {
	return len(text) / 4
}

// EstimateTokensAccurate provides a slightly more accurate estimate.
// Uses ~3.5 chars per token for English text.
func EstimateTokensAccurate(text string) int {
	// Count words and characters
	words := 0
	inWord := false
	for _, c := range text {
		if c == ' ' || c == '\n' || c == '\t' {
			if inWord {
				words++
				inWord = false
			}
		} else {
			inWord = true
		}
	}
	if inWord {
		words++
	}
	
	// Tokens are roughly: words * 1.3 (accounting for subword tokenization)
	// But also account for special characters and whitespace
	return int(float64(words)*1.3) + len(text)/20
}

